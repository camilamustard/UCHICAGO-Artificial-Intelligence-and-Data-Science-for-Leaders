{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df=pd.read_csv('creditcard.csv',nrows=100000)\n",
    "#df.to_csv('creditsample.csv')\n",
    "df=pd.read_csv('creditsample.csv',index_col=[0])\n",
    "df.head(30)\n",
    "Time\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\tV9\t...\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\tClass\n",
    "0\t0\t-1.359807\t-0.072781\t2.536347\t1.378155\t-0.338321\t0.462388\t0.239599\t0.098698\t0.363787\t...\t-0.018307\t0.277838\t-0.110474\t0.066928\t0.128539\t-0.189115\t0.133558\t-0.021053\t149.62\t0\n",
    "1\t0\t1.191857\t0.266151\t0.166480\t0.448154\t0.060018\t-0.082361\t-0.078803\t0.085102\t-0.255425\t...\t-0.225775\t-0.638672\t0.101288\t-0.339846\t0.167170\t0.125895\t-0.008983\t0.014724\t2.69\t0\n",
    "2\t1\t-1.358354\t-1.340163\t1.773209\t0.379780\t-0.503198\t1.800499\t0.791461\t0.247676\t-1.514654\t...\t0.247998\t0.771679\t0.909412\t-0.689281\t-0.327642\t-0.139097\t-0.055353\t-0.059752\t378.66\t0\n",
    "3\t1\t-0.966272\t-0.185226\t1.792993\t-0.863291\t-0.010309\t1.247203\t0.237609\t0.377436\t-1.387024\t...\t-0.108300\t0.005274\t-0.190321\t-1.175575\t0.647376\t-0.221929\t0.062723\t0.061458\t123.50\t0\n",
    "4\t2\t-1.158233\t0.877737\t1.548718\t0.403034\t-0.407193\t0.095921\t0.592941\t-0.270533\t0.817739\t...\t-0.009431\t0.798278\t-0.137458\t0.141267\t-0.206010\t0.502292\t0.219422\t0.215153\t69.99\t0\n",
    "5\t2\t-0.425966\t0.960523\t1.141109\t-0.168252\t0.420987\t-0.029728\t0.476201\t0.260314\t-0.568671\t...\t-0.208254\t-0.559825\t-0.026398\t-0.371427\t-0.232794\t0.105915\t0.253844\t0.081080\t3.67\t0\n",
    "6\t4\t1.229658\t0.141004\t0.045371\t1.202613\t0.191881\t0.272708\t-0.005159\t0.081213\t0.464960\t...\t-0.167716\t-0.270710\t-0.154104\t-0.780055\t0.750137\t-0.257237\t0.034507\t0.005168\t4.99\t0\n",
    "7\t7\t-0.644269\t1.417964\t1.074380\t-0.492199\t0.948934\t0.428118\t1.120631\t-3.807864\t0.615375\t...\t1.943465\t-1.015455\t0.057504\t-0.649709\t-0.415267\t-0.051634\t-1.206921\t-1.085339\t40.80\t0\n",
    "8\t7\t-0.894286\t0.286157\t-0.113192\t-0.271526\t2.669599\t3.721818\t0.370145\t0.851084\t-0.392048\t...\t-0.073425\t-0.268092\t-0.204233\t1.011592\t0.373205\t-0.384157\t0.011747\t0.142404\t93.20\t0\n",
    "9\t9\t-0.338262\t1.119593\t1.044367\t-0.222187\t0.499361\t-0.246761\t0.651583\t0.069539\t-0.736727\t...\t-0.246914\t-0.633753\t-0.120794\t-0.385050\t-0.069733\t0.094199\t0.246219\t0.083076\t3.68\t0\n",
    "10\t10\t1.449044\t-1.176339\t0.913860\t-1.375667\t-1.971383\t-0.629152\t-1.423236\t0.048456\t-1.720408\t...\t-0.009302\t0.313894\t0.027740\t0.500512\t0.251367\t-0.129478\t0.042850\t0.016253\t7.80\t0\n",
    "11\t10\t0.384978\t0.616109\t-0.874300\t-0.094019\t2.924584\t3.317027\t0.470455\t0.538247\t-0.558895\t...\t0.049924\t0.238422\t0.009130\t0.996710\t-0.767315\t-0.492208\t0.042472\t-0.054337\t9.99\t0\n",
    "12\t10\t1.249999\t-1.221637\t0.383930\t-1.234899\t-1.485419\t-0.753230\t-0.689405\t-0.227487\t-2.094011\t...\t-0.231809\t-0.483285\t0.084668\t0.392831\t0.161135\t-0.354990\t0.026416\t0.042422\t121.50\t0\n",
    "13\t11\t1.069374\t0.287722\t0.828613\t2.712520\t-0.178398\t0.337544\t-0.096717\t0.115982\t-0.221083\t...\t-0.036876\t0.074412\t-0.071407\t0.104744\t0.548265\t0.104094\t0.021491\t0.021293\t27.50\t0\n",
    "14\t12\t-2.791855\t-0.327771\t1.641750\t1.767473\t-0.136588\t0.807596\t-0.422911\t-1.907107\t0.755713\t...\t1.151663\t0.222182\t1.020586\t0.028317\t-0.232746\t-0.235557\t-0.164778\t-0.030154\t58.80\t0\n",
    "15\t12\t-0.752417\t0.345485\t2.057323\t-1.468643\t-1.158394\t-0.077850\t-0.608581\t0.003603\t-0.436167\t...\t0.499625\t1.353650\t-0.256573\t-0.065084\t-0.039124\t-0.087086\t-0.180998\t0.129394\t15.99\t0\n",
    "16\t12\t1.103215\t-0.040296\t1.267332\t1.289091\t-0.735997\t0.288069\t-0.586057\t0.189380\t0.782333\t...\t-0.024612\t0.196002\t0.013802\t0.103758\t0.364298\t-0.382261\t0.092809\t0.037051\t12.99\t0\n",
    "17\t13\t-0.436905\t0.918966\t0.924591\t-0.727219\t0.915679\t-0.127867\t0.707642\t0.087962\t-0.665271\t...\t-0.194796\t-0.672638\t-0.156858\t-0.888386\t-0.342413\t-0.049027\t0.079692\t0.131024\t0.89\t0\n",
    "18\t14\t-5.401258\t-5.450148\t1.186305\t1.736239\t3.049106\t-1.763406\t-1.559738\t0.160842\t1.233090\t...\t-0.503600\t0.984460\t2.458589\t0.042119\t-0.481631\t-0.621272\t0.392053\t0.949594\t46.80\t0\n",
    "19\t15\t1.492936\t-1.029346\t0.454795\t-1.438026\t-1.555434\t-0.720961\t-1.080664\t-0.053127\t-1.978682\t...\t-0.177650\t-0.175074\t0.040002\t0.295814\t0.332931\t-0.220385\t0.022298\t0.007602\t5.00\t0\n",
    "20\t16\t0.694885\t-1.361819\t1.029221\t0.834159\t-1.191209\t1.309109\t-0.878586\t0.445290\t-0.446196\t...\t-0.295583\t-0.571955\t-0.050881\t-0.304215\t0.072001\t-0.422234\t0.086553\t0.063499\t231.71\t0\n",
    "21\t17\t0.962496\t0.328461\t-0.171479\t2.109204\t1.129566\t1.696038\t0.107712\t0.521502\t-1.191311\t...\t0.143997\t0.402492\t-0.048508\t-1.371866\t0.390814\t0.199964\t0.016371\t-0.014605\t34.09\t0\n",
    "22\t18\t1.166616\t0.502120\t-0.067300\t2.261569\t0.428804\t0.089474\t0.241147\t0.138082\t-0.989162\t...\t0.018702\t-0.061972\t-0.103855\t-0.370415\t0.603200\t0.108556\t-0.040521\t-0.011418\t2.28\t0\n",
    "23\t18\t0.247491\t0.277666\t1.185471\t-0.092603\t-1.314394\t-0.150116\t-0.946365\t-1.617935\t1.544071\t...\t1.650180\t0.200454\t-0.185353\t0.423073\t0.820591\t-0.227632\t0.336634\t0.250475\t22.75\t0\n",
    "24\t22\t-1.946525\t-0.044901\t-0.405570\t-1.013057\t2.941968\t2.955053\t-0.063063\t0.855546\t0.049967\t...\t-0.579526\t-0.799229\t0.870300\t0.983421\t0.321201\t0.149650\t0.707519\t0.014600\t0.89\t0\n",
    "25\t22\t-2.074295\t-0.121482\t1.322021\t0.410008\t0.295198\t-0.959537\t0.543985\t-0.104627\t0.475664\t...\t-0.403639\t-0.227404\t0.742435\t0.398535\t0.249212\t0.274404\t0.359969\t0.243232\t26.43\t0\n",
    "26\t23\t1.173285\t0.353498\t0.283905\t1.133563\t-0.172577\t-0.916054\t0.369025\t-0.327260\t-0.246651\t...\t0.067003\t0.227812\t-0.150487\t0.435045\t0.724825\t-0.337082\t0.016368\t0.030041\t41.88\t0\n",
    "27\t23\t1.322707\t-0.174041\t0.434555\t0.576038\t-0.836758\t-0.831083\t-0.264905\t-0.220982\t-1.071425\t...\t-0.284376\t-0.323357\t-0.037710\t0.347151\t0.559639\t-0.280158\t0.042335\t0.028822\t16.00\t0\n",
    "28\t23\t-0.414289\t0.905437\t1.727453\t1.473471\t0.007443\t-0.200331\t0.740228\t-0.029247\t-0.593392\t...\t0.077237\t0.457331\t-0.038500\t0.642522\t-0.183891\t-0.277464\t0.182687\t0.152665\t33.00\t0\n",
    "29\t23\t1.059387\t-0.175319\t1.266130\t1.186110\t-0.786002\t0.578435\t-0.767084\t0.401046\t0.699500\t...\t0.013676\t0.213734\t0.014462\t0.002951\t0.294638\t-0.395070\t0.081461\t0.024220\t12.99\t0\n",
    "30 rows Ã— 31 columns\n",
    "\n",
    "First question to ask in Anomaly Detection is: \"Do we have labeled data? In other words, do we have any previous observations we detected as fraud in our data?\n",
    "If yes, what's next?\n",
    "\n",
    "If no, what's next?\n",
    "\n",
    "df['Class'].describe()\n",
    "count    100000.00000\n",
    "mean          0.00223\n",
    "std           0.04717\n",
    "min           0.00000\n",
    "25%           0.00000\n",
    "50%           0.00000\n",
    "75%           0.00000\n",
    "max           1.00000\n",
    "Name: Class, dtype: float64\n",
    "df['Class'].value_counts()\n",
    "0    99777\n",
    "1      223\n",
    "Name: Class, dtype: int64\n",
    "# Explore the features available in your dataframe\n",
    "print(df.info())\n",
    "\n",
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = df['Class'].value_counts()\n",
    "print(occ)\n",
    "\n",
    "# Print the ratio of fraud cases\n",
    "print(occ / len(df.index))\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 100000 entries, 0 to 99999\n",
    "Data columns (total 31 columns):\n",
    "Time      100000 non-null int64\n",
    "V1        100000 non-null float64\n",
    "V2        100000 non-null float64\n",
    "V3        100000 non-null float64\n",
    "V4        100000 non-null float64\n",
    "V5        100000 non-null float64\n",
    "V6        100000 non-null float64\n",
    "V7        100000 non-null float64\n",
    "V8        100000 non-null float64\n",
    "V9        100000 non-null float64\n",
    "V10       100000 non-null float64\n",
    "V11       100000 non-null float64\n",
    "V12       100000 non-null float64\n",
    "V13       100000 non-null float64\n",
    "V14       100000 non-null float64\n",
    "V15       100000 non-null float64\n",
    "V16       100000 non-null float64\n",
    "V17       100000 non-null float64\n",
    "V18       100000 non-null float64\n",
    "V19       100000 non-null float64\n",
    "V20       100000 non-null float64\n",
    "V21       100000 non-null float64\n",
    "V22       100000 non-null float64\n",
    "V23       100000 non-null float64\n",
    "V24       100000 non-null float64\n",
    "V25       100000 non-null float64\n",
    "V26       100000 non-null float64\n",
    "V27       100000 non-null float64\n",
    "V28       100000 non-null float64\n",
    "Amount    100000 non-null float64\n",
    "Class     100000 non-null int64\n",
    "dtypes: float64(29), int64(2)\n",
    "memory usage: 24.4 MB\n",
    "None\n",
    "0    99777\n",
    "1      223\n",
    "Name: Class, dtype: int64\n",
    "0    0.99777\n",
    "1    0.00223\n",
    "Name: Class, dtype: float64\n",
    "# Define a function to create a scatter plot of our data and labels\n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "def plot_data(X, y):\n",
    "\tplt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "\tplt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='y')\n",
    "\tplt.legend()\n",
    "\treturn plt.show()\n",
    "\n",
    "# Create X and y from our above defined function\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df.iloc[:,-1].values\n",
    "X=df.iloc[:, :-1].values\n",
    "\n",
    "\n",
    "# Plot our data by running our plot data function on X and y\n",
    "plot_data(X, y)\n",
    "\n",
    "Yes we have some labels\n",
    "223 data labels -out of 100K- sufficient enough to run a classification learning algorithm?\n",
    "'''\n",
    "We applied an upsampling technique to balance minory class in our data\n",
    "Almost doubled the size of our data by generating generic samples\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "\n",
    "# Define the resampling method\n",
    "method = SMOTE(kind='regular')\n",
    "\n",
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.fit_sample(X, y)\n",
    "\n",
    "# Plot the resampled data\n",
    "plot_data(X_resampled, y_resampled)\n",
    "\n",
    "Conventional Rule-Based Statistical Methods\n",
    "# Run a groupby command on our labels and obtain the mean for each feature\n",
    "df.groupby('Class').mean()\n",
    "\n",
    "# Implement a rule for stating which cases are flagged as fraud\n",
    "df['flag_as_fraud'] = np.where(np.logical_and(df['V1'] < -3, df['V3'] < -5), 1, 0)\n",
    "\n",
    "# Create a crosstab of flagged fraud cases versus the actual fraud cases\n",
    "print(pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))\n",
    "Flagged Fraud      0    1\n",
    "Actual Fraud             \n",
    "0              99374  403\n",
    "1                134   89\n",
    "a=df.groupby('Class').mean()\n",
    "a\n",
    "Time\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\tV9\t...\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\tflag_as_fraud\n",
    "Class\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "0\t94838.202258\t0.008258\t-0.006271\t0.012171\t-0.007860\t0.005453\t0.002419\t0.009637\t-0.000987\t0.004467\t...\t-0.001235\t-0.000024\t0.000070\t0.000182\t-0.000072\t-0.000089\t-0.000295\t-0.000131\t88.291022\t0.004312\n",
    "1\t80746.806911\t-4.771948\t3.623778\t-7.033281\t4.542029\t-3.151225\t-1.397737\t-5.568731\t0.570636\t-2.581123\t...\t0.713588\t0.014049\t-0.040308\t-0.105130\t0.041449\t0.051648\t0.170575\t0.075667\t122.211321\t0.345528\n",
    "2 rows Ã— 31 columns\n",
    "\n",
    "Machine Learning on Imbalanced Data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "y = df.iloc[:,-1].values\n",
    "X=df.iloc[:, :-1].values\n",
    "# Create the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit a logistic regression model to our data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain model predictions\n",
    "predicted = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "# Print the classifcation report and confusion matrix\n",
    "#print('Classification report:\\n', metrics.classification_report(y_test, predicted))\n",
    "conf_mat = metrics.confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print(classification_report(y_test, predicted))\n",
    "print('Confusion matrix:\\n', conf_mat)\n",
    "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
    "  FutureWarning)\n",
    "0.9725126081293209\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     29941\n",
    "           1       0.76      0.64      0.70        59\n",
    "\n",
    "   micro avg       1.00      1.00      1.00     30000\n",
    "   macro avg       0.88      0.82      0.85     30000\n",
    "weighted avg       1.00      1.00      1.00     30000\n",
    "\n",
    "Confusion matrix:\n",
    " [[29929    12]\n",
    " [   21    38]]\n",
    "import numpy as np\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x11d22aa50>\n",
    "\n",
    "Machine Learning with improved balance\n",
    "# This is the pipeline module we need for this from imblearn\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE\n",
    "#https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#smote-adasyn\n",
    "# Define which resampling method and which ML model to use in the pipeline\n",
    "resampling = SMOTE(kind='borderline2')\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])\n",
    "# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \n",
    "pipeline.fit(X_train, y_train) \n",
    "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
    "  FutureWarning)\n",
    "Pipeline(memory=None,\n",
    "     steps=[('SMOTE', SMOTE(k_neighbors=5, kind='borderline-2', m_neighbors=10, n_jobs=1,\n",
    "   out_step='deprecated', random_state=None, ratio=None,\n",
    "   sampling_strategy='auto', svm_estimator='deprecated')), ('Logistic Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False))])\n",
    "predicted = model.predict(X_test)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "\n",
    "'''\n",
    "Logistic Regression Results for Resampled Data (~200K samples instead of 100K)\n",
    "\n",
    "'''\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x13faab450>\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "y_true=y_test\n",
    "y_pred=predicted\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_true, y_pred, labels)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "'''\n",
    "Random Forest Results for Resampled Data (~200K samples instead of 100K)\n",
    "\n",
    "'''\n",
    "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
    "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x140f60250>\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=0.5, kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "y_true=y_test\n",
    "y_pred=predicted\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_true, y_pred, labels)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "'''\n",
    "Support Vector Machine results for Resampled Data (~200K samples instead of 100K)\n",
    "'''\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x141126dd0>\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "model = DecisionTreeClassifier(max_depth=6, random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=0.5, kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "y_true=y_test\n",
    "y_pred=predicted\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_true, y_pred, labels)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "'''\n",
    "Decision Tree Classifier results for Resampled Data (~200K samples instead of 100K)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x11d2bf710>\n",
    "\n",
    "Calculate Precision and Recall for each model\n",
    "Compare the Confusion Matrices for each model and determine which model will be a better fit for this business problem\n",
    "Linear & Non-linear Decision Boundaries\n",
    "import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "image = PILImage.open(\"monns.png\")\n",
    "\n",
    "output = io.BytesIO()\n",
    "image.save(output, format='PNG')\n",
    "encoded_string = base64.b64encode(output.getvalue()).decode()\n",
    "\n",
    "html = '<img src=\"data:monns.png;base64,{}\"/>'.format(encoded_string)\n",
    "IPython.display.HTML(html)\n",
    "\n",
    "import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "image = PILImage.open(\"irs.png\")\n",
    "\n",
    "output = io.BytesIO()\n",
    "image.save(output, format='PNG')\n",
    "encoded_string = base64.b64encode(output.getvalue()).decode()\n",
    "\n",
    "html = '<img src=\"data:irs.png;base64,{}\"/>'.format(encoded_string)\n",
    "IPython.display.HTML(html)\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(X_resampled)\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_features = pca.fit_transform(X_resampled)\n",
    "\n",
    "# Assign 0th column of pca_features: xs\n",
    "xs = pca_features[:,0]\n",
    "\n",
    "# Assign 1st column of pca_features: ys\n",
    "ys = pca_features[:,1]\n",
    "plt.scatter(xs, ys)\n",
    "plt.axis()\n",
    "plt.show()\n",
    "\n",
    "orthogonal\n",
    "\n",
    "Based on decision boundary shapes above, we think either decision tree or random forest decision boundaries would do a good work\n",
    "We may be biased though since precision/recall and confusion matrix results also showed the strength of Random Forest\n",
    "# Count the total number of observations from the length of y\n",
    "total_obs = len(y)\n",
    "\n",
    "# Count the total number of non-fraudulent observations \n",
    "non_fraud = [i for i in y if i == 0]\n",
    "count_non_fraud = non_fraud.count(0)\n",
    "\n",
    "# Calculate the percentage of non fraud observations in the dataset\n",
    "percentage = (float(count_non_fraud)/float(total_obs)) * 100\n",
    "\n",
    "# Print the percentage: this is our \"natural accuracy\" by doing nothing\n",
    "print(percentage)\n",
    "99.777\n",
    "With Random Forests you create random subsets of the features and build smaller trees using these subsets. Afterwards, the algorithm combines the subtrees of subsamples of features, and averages the results.\n",
    "\n",
    "# Import the packages to get the different performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Obtain the predictions from our random forest model \n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "#print(classification_report(y_test, predicted))\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
    "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
    "0.9829548960413107\n",
    "[[29938     3]\n",
    " [    3    56]]\n",
    "We monitor Random Forest algorithm provided less accuracy than natural guess by using a general performance accuracy metric. However, it seems more reliable to us and performs significantly better than a traditional rule-based approach\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
